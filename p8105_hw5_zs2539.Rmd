---
title: "Homework 5 solutions"
author: "Zhengwei Song"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

* The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r, warning = FALSE, message=FALSE}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

* The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r, warning = FALSE, message=FALSE}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

* Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r, warning = FALSE, message=FALSE}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

* This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

&nbsp;

## Problem 2

```{r, warning = FALSE, message=FALSE}
# Import the homicide dataset
homicide_raw = read_csv("./data/homicide-data.csv")
head(homicide_raw)
```

* Homicide data collected by the Washington Post for 50 major U.S. cities reported `r homicide_raw %>% nrow()` criminal homicides for a ten-year period, including `r homicide_raw %>% ncol()` variables, such as `r homicide_raw %>% names`.

* The `reported_date`, longitude `lon`, latitude `lat` were noted as numeric variables, while `age` is a character variable, and others as character variables.

* Also, there are `r sum(is.na(homicide_raw$lat))` missing values of latitude and `r sum(is.na(homicide_raw$lon))` of longitude. Also, the city Tulsa, OK was incorrectly labeled in AL, and this observation will be dropped from subsequent data analysis.

* In addition, some entries for victim race, age, and sex were reported as unknown.

```{r, warning = FALSE, message=FALSE}
# Clean dataset, create the variable city_state
# mutate the disposition data as solved or not
homicide_df = homicide_raw %>% 
    janitor::clean_names() %>%
    mutate(reported_date =
               as.Date(as.character(reported_date), format = "%Y%m%d"),
           city_state = str_c(city, state, sep = ", "),
           resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved",
    )) %>%
    relocate(city_state) %>%
    filter(city_state != "Tulsa, AL")
```

```{r, warning = FALSE, message=FALSE}
# summarize within cities to obtain the total number of homicides and the number of unsolved homicides
summary_hom_df = homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_unsolved = sum(resolution == "unsolved"),
    hom_total = n()
  )
summary_hom_df %>% 
  knitr::kable(align = "lrr", 
               col.names = c("City", "Number of unsovled homicides", "Total number of homicides"))
```

```{r, warning = FALSE, message=FALSE}
# estimate the proportion of homicides that are unsolved, 
# save the output as an R object, 
# pull the estimated proportion and confidence intervals
prop_baltimore = prop.test(
  summary_hom_df %>% filter(city_state == "Baltimore, MD") %>% pull(hom_unsolved), 
  summary_hom_df %>% filter(city_state == "Baltimore, MD") %>% pull(hom_total)) 
prop_baltimore %>% broom::tidy()
```

```{r, warning = FALSE, message=FALSE}
# iterate to estimate the proportion of unsolved homicides in all cities
prop_cities = 
  summary_hom_df %>% 
  mutate(
    prop_tests = purrr::map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = purrr::map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

```{r, warning = FALSE, message=FALSE}
# Create a plot showing the estimates and CIs for each city
prop_cities %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Estimates and 95% CIs of Proportion of Unsolved Homicides in All Cities",
    x = "City",
    y = "Estimate")
```

&nbsp;

## Problem 3
```{r, warning = FALSE, message=FALSE}
# Construct function for one-sample t-test with n=30, sigma=5
t_test = function(n = 30, mu, sigma = 5){
    
  sim_data = tibble(x = rnorm(n, mean = mu, sd = sigma)) 
    
  test_data = t.test(sim_data, mu = 0, conf.level = 0.95)
  
  sim_data %>% 
    summarize(
      estimate = pull(broom::tidy(test_data), estimate),
      p_value = pull(broom::tidy(test_data), p.value)
    )
}

# Generate 5000 datasets from the model and repeat the t-test for mu = 0,1,2,3,4,5,6
set.seed(2022)
sim_result_df = 
  tibble(mu = c(0:6)) %>% 
  mutate(
    output_list = map(.x = mu, ~rerun(5000, t_test(mu = .x))),
    estimate_df = map(output_list, bind_rows)
    ) %>% 
  select(-output_list) %>% 
  unnest(estimate_df)
```

```{r, warning = FALSE, message=FALSE}
# Make a plot showing the proportion of times the null was rejected on the y axis and the true value of mu on the x axis
sim_result_df  %>% 
  group_by(mu) %>%
  filter(p_value < 0.05) %>% 
  summarize(rej_num = n(), rej_prop = rej_num/5000) %>% 
  ggplot(aes(x = mu, y = rej_prop)) + 
  geom_point(alpha = .8) +
  geom_line(alpha = .8) +
  geom_text(aes(label = round(rej_prop, 3)), vjust = -1, size = 3) + 
  scale_x_continuous(limits = c(0,6), breaks = seq(0, 6, 1)) +
  scale_y_continuous(limits = c(0,1), breaks = seq(0, 1, .2)) +
  labs(
    title = "Association between effect size and power of the t-test",
    x = "True mean",
    y = "Proportion of times the null was rejected")
```

* As the true mean increases from 0 to 6, the proportion of times the null hypothesis is rejected increases accordingly (from 0.051 until 1). Also, the proportion of times the null was rejected (the power of the test) growth becomes slower when the true mean is sufficiently different from 0 (true mean â‰¥ 3). Thus, the larger effect size, the greater the power, i.e., they are positively correlated.

```{r, warning=FALSE, message=FALSE}
# Make a plot showing the average estimate of mu on the y axis and the true value of mu on the x axis
sim_result_df %>% 
  group_by(mu) %>% 
  summarise(avg_estimate = mean(estimate)) %>% 
  ggplot(aes(x = mu, y = avg_estimate)) +
  geom_point(alpha = .8) +
  geom_line(alpha = .8) +
  geom_text(aes(label = round(avg_estimate,2)), vjust = -1, size = 3) + 
  scale_x_continuous(limits = c(0,6.5), breaks = seq(0,6,1)) +
  scale_y_continuous(limits = c(-0.1,6.5), breaks = seq(0,6,1)) +
  labs(
    title = "Association between average estimate mean and true mean",
    x = "True mean",
    y = "Average estimate mean"
  ) 

# Make a second plot the average estimate of mu in null-rejected samples on the y axis and the true value of mu on the x axis
sim_rej = sim_result_df %>% 
  filter(p_value < 0.05) %>% 
  group_by(mu) %>% 
  summarise(avg_estimate = mean(estimate)) 

sim_result_df %>% 
  group_by(mu) %>% 
  summarise(avg_estimate = mean(estimate)) %>% 
  ggplot(aes(x = mu, y = avg_estimate, color = "Total samples")) +
  geom_point() +
  geom_line() + 
  geom_text(aes(label = round(avg_estimate,2)), vjust = 2, size = 3) + 
  geom_point(data = sim_rej, aes(color = "Rejected samples")) +
  geom_line(data = sim_rej, aes(x = mu, y = avg_estimate, color = "Rejected samples")) + 
  geom_text(data = sim_rej, aes(label = round(avg_estimate,2), color = "Rejected samples"), vjust = -1, size = 3) + 
  scale_x_continuous(limits = c(0,6.5), breaks = seq(0,6,1)) +
  scale_y_continuous(limits = c(-0.5,6.5), breaks = seq(0,6,1)) +
  labs(x = "True mean",
       y = "Average estimate mean",
       title = "Association between average estimate mean and true mean",
       color = "Type") +
  scale_color_manual(values = c("Total samples" = "#7B68EE", "Rejected samples" = "#C71585"))
```

* For the total samples, the average estimated mean $\hat{\mu}$ is almost equal to the true mean. While for the rejected samples, the average estimated mean $\hat{\mu}$ is obviously higher than the true mean $\mu$ when the true mean $\mu$ is close to 0 ($\mu$<3).

* The reason is that the probability of rejecting the null hypothesis increases with the increase of effect size. When samples with higher true means $\mu$ are simulated, larger detectable effects were obtained, i.e. a rise in statistical power, resulting in a boost concerning correctness of rejecting the null, 
