---
title: "Homework 5 solutions"
author: "Zhengwei Song"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

* The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r, warning = FALSE, message=FALSE}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

* The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r, warning = FALSE, message=FALSE}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

* Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r, warning = FALSE, message=FALSE}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

* This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

&nbsp;

## Problem 2

#### Importing the homicide dataset
```{r, warning = FALSE, message=FALSE}
homicide_raw = read_csv("./data/homicide-data.csv")
head(homicide_raw)
```

* Homicide data collected by the Washington Post for 50 major U.S. cities reported `r nrow(homicide_raw)` criminal homicides for a ten-year period, including `r ncol(homicide_raw)` variables, such as `r names(homicide_raw)`.

* The `reported_date`, longitude `lon`, latitude `lat` were noted as numeric variables, while `age` is a character variable, and others as character variables.

* Also, there are `r sum(is.na(homicide_raw$lat))` missing values of latitude and `r sum(is.na(homicide_raw$lon))` of longitude. Also, the city Tulsa, OK was incorrectly labeled in AL, and this observation will be dropped from subsequent data analysis.

* In addition, some entries for victim race, age, and sex were reported as unknown.

#### Cleaning dataset, creating the variable `city_state` and arranging the column `disposition` so we can read the number of total and unsolved murders by city-state
```{r, warning = FALSE, message=FALSE}
homicide_df = homicide_raw %>% 
    janitor::clean_names() %>%
    mutate(
        reported_date = as.Date(as.character(reported_date), format = "%Y%m%d"),
        city_state = str_c(city, state, sep = ", "),
        resolution = case_when(
            disposition == "Closed without arrest" ~ "unsolved",
            disposition == "Open/No arrest" ~ "unsolved",
            disposition == "Closed by arrest" ~ "solved",
    )) %>%
    relocate(city_state) %>%
    filter(city_state != "Tulsa, AL")
```

#### Summarizing within cities to obtain the total number of homicides and the number of unsolved homicides
```{r, warning = FALSE, message=FALSE}
city_homicide_df = homicide_df %>%
    group_by(city_state) %>%
    summarize(
        hom_unsolved = sum(resolution == "unsolved"),
        hom_total = n()
  )

city_homicide_df %>% knitr::kable(col.names = c("City", "Unsolved Murders", "Total Murders"))
```

#### Estimating and saving the proportion of homicides that are unsolved in Baltimore, MD, and pulling the estimated proportion and confidence intervals
```{r, warning = FALSE, message=FALSE}
baltimore_df = homicide_df %>%
  filter(city_state == "Baltimore, MD")

baltimore_summary = baltimore_df %>% 
  summarize(
    unsolved = sum(resolution == "unsolved"),
    n = n()
  )

baltimore_test = prop.test(
    x = baltimore_summary %>% pull(unsolved),
    n = baltimore_summary %>% pull(n))

baltimore_test %>% broom::tidy()
```

#### Iterating to estimate the proportion of unsolved homicides in all cities
```{r, warning = FALSE, message=FALSE}
prop_cities = city_homicide_df %>% 
  mutate(
    prop_tests = purrr::map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = purrr::map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)

head(prop_cities)
```

#### Creating a plot showing the estimates and CIs for each city
```{r}
prop_cities %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Estimated proportions and 95% CIs of unsolved murders by city",
    x = "City, state",
    y = "Estimate")
```

&nbsp;

## Problem 3
#### Constructing the function for one-sample t-test
```{r, warning = FALSE, message=FALSE}
t_test = function(n = 30, mu, sigma = 5){

    sim_data = tibble(x = rnorm(n, mean = mu, sd = sigma)) 
    
    test_data = t.test(sim_data, mu = 0, conf.level = 0.95)
  
    sim_data %>% summarize(
      estimated_mu = pull(broom::tidy(test_data), estimate),
      p_value = pull(broom::tidy(test_data), p.value)
    )
}
```

#### Generating 5000 datasets from the model and repeating the t-test for $\mu$ = 0,1,2,3,4,5,6
```{r, warning = FALSE, message=FALSE}
set.seed(2022)
sim_results_df = 
  tibble(mu = c(0:6)) %>% 
  mutate(
    output_list = map(.x = mu, ~rerun(5000, t_test(mu = .x))),
    estimate_df = map(output_list, bind_rows)
    ) %>% 
  select(-output_list) %>% 
  unnest(estimate_df)
```

#### Plot showing the proportion of times the null was rejected vs the true mean $\mu$
```{r, warning = FALSE, message=FALSE}
sim_results_df  %>% 
  group_by(mu) %>%
  filter(p_value < 0.05) %>% 
  summarize(rej_count = n(), rej_prop = rej_count/5000) %>% 
  ggplot(aes(x = mu, y = rej_prop)) + 
  geom_point() +
  geom_line() +
  geom_text(aes(label = round(rej_prop, 3)), vjust = -1, size = 3) + 
  scale_x_continuous(n.breaks = 8) +
  scale_y_continuous(n.breaks = 8, limits = c(-0.01,1.03)) +
  labs(
    title = "Association between effect size and power of the t-test",
    x = "True mean",
    y = "Proportion of rejecting the null")
```

* As the true mean increases from 0 to 6, the proportion of times the null hypothesis is rejected increases accordingly (from 0.051 until 1). Also, the proportion of times the null was rejected (the power of the test) growth becomes slower when the true mean is sufficiently different from 0 (true mean â‰¥ 3). Thus, the larger effect size, the greater the power, i.e., they are positively correlated.

#### Plot showing the average estimated $\hat{\mu}$ vs the true mean $\mu$ in total and null-rejected samples
```{r, warning=FALSE, message=FALSE}
sim_results_df %>% 
  group_by(mu) %>% 
  summarize(avg_estimated_mu = mean(estimated_mu)) %>% 
  ggplot(aes(x = mu, y = avg_estimated_mu)) +
  geom_point() +
  geom_line() +
  geom_text(aes(label = round(avg_estimated_mu,3)), vjust = -1, size = 3) + 
  scale_x_continuous(n.breaks = 7) +
  scale_y_continuous(n.breaks = 7) +
  labs(
    title = "Association between average estimated mean and true mean",
    x = "True mean",
    y = "Average estimate mean"
  ) 
```

```{r, warning=FALSE, message=FALSE}
sim_rej = sim_results_df %>% 
  filter(p_value < 0.05) %>% 
  group_by(mu) %>% 
  summarise(avg_estimated_mu = mean(estimated_mu)) 

sim_results_df %>% 
  group_by(mu) %>% 
  summarise(avg_estimated_mu = mean(estimated_mu)) %>% 
  ggplot(aes(x = mu, y = avg_estimated_mu, color = "Total samples")) +
  geom_point() +
  geom_line() + 
  geom_text(aes(label = round(avg_estimated_mu,2)), vjust = 2, size = 3) + 
  geom_point(data = sim_rej, aes(color = "Rejected samples")) +
  geom_line(data = sim_rej, aes(x = mu, y = avg_estimated_mu, color = "Rejected samples")) + 
  geom_text(data = sim_rej, aes(label = round(avg_estimated_mu,3), color = "Rejected samples"), vjust = -1, size = 3) + 
  scale_x_continuous(n.breaks = 7) +
  scale_y_continuous(n.breaks = 7, limits = c(-0.5,6.5)) +
  labs(x = "True mean",
       y = "Average estimate mean",
       title = "Association between average estimated mean and true mean",
       color = "Type") +
  scale_color_manual(values = c("Total samples" = "black", "Rejected samples" = "red"))
```

* For the total samples, the average estimated mean $\hat{\mu}$ is almost equal to the true mean. While for the rejected samples, the average estimated mean $\hat{\mu}$ is obviously higher than the true mean $\mu$ when the true mean $\mu$ is close to 0 ( $\mu$<3).

* The reason is that the probability of rejecting the null hypothesis increases with the increase of effect size. When samples with higher true means $\mu$ are simulated, larger detectable effects were obtained, i.e. a rise in statistical power, resulting in a boost concerning correctness of rejecting the null, 
